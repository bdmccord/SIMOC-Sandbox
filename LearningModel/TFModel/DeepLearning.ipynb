{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Modeling for SIMOC\n",
    "\n",
    "Deep learning is a way of modeling a system by creating a set of tasks that a computer follows to learn about the system. Given the large amounts of data necessary to make these models work, deep learning has become extremely popular in the past few years and has been very successful at breaking down complex systems. \n",
    "\n",
    "Deep learning is ideal for SIMOC as it allows for a model to be built upon real data and could help us learn more about how a self-reliant colony may respond when given new tasks. To demonstrate how deep learning works, data from a pre-built model will be used to train and test this new model. The input will be oxygen and carbon dioxide levels and based on this input the model will try to predict how many people and square meters of plants were in that room at the time that that dataset was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll load in the necessary libraries. TensorFlow is the deep learning library that will be used to do the bulk of the model building and Numpy will be used for some minor array manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data needs to be loaded in and separated into training and testing datasets. For simplicity, I have created training and testing datasets in CSV files in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = np.genfromtxt('trainData.csv', delimiter=',')\n",
    "testData = np.genfromtxt('testData.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, the testing and training data will be separated into inputs and outputs that will be trained and tested against once the model is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = trainData[0:,0:23]\n",
    "train_y = trainData[0:,[24,25]]\n",
    "test_x = testData[0:,0:23]\n",
    "test_y = testData[0:,[24,25]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded in, we can start defining the model. Solving this problem could likely only require linear regression, so using a deep learning model is a little overkill and it will probably lead to some overfitting. However, since the final model will be using deep learning, using a deep learning problem for this data will most likely be accurate enough.\n",
    "\n",
    "For this model, two hidden layers will be used (so it is technically a deep learning model) with 4 nodes each. The final solution will have 2 outputs, the number of plants and number of people, and the batch size will be set at 100 sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_hl1 = 4\n",
    "nodes_hl2 = 4\n",
    "n_classes = 2\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the input and output of this model, two tensorflow placeholder variables will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, 23])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can continue to define the model. As mentioned before, this model will have two hidden layers each initialized with their own weights and biases set randomly at when created. These weights and biases will be adjusted over the course the of model's lifetime. The model then multiplies the given data by the weights and adds the bias to generate a projected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(data):\n",
    "    hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([23,nodes_hl1],seed=1)),\n",
    "                        'bias': tf.Variable(tf.random_normal([nodes_hl1],seed=1))}\n",
    "    hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([nodes_hl1,nodes_hl2],seed=1)),\n",
    "                        'bias': tf.Variable(tf.random_normal([nodes_hl2],seed=1))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([nodes_hl2,n_classes],seed=1)),\n",
    "                        'bias': tf.Variable(tf.random_normal([n_classes],seed=1))}\n",
    "\n",
    "    # (data * weights) + bias\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data, hidden_layer_1['weights']), hidden_layer_1['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    # Activation Function\n",
    "    output = tf.add(tf.matmul(l2, output_layer['weights']), output_layer['bias'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training the model. To do this, a cost function is created based on the predictions and expected values. This cost function is then adjusted by tensorflow and weights are adjusted to produce a close estimate to our expected output. This cycle happens for each batch of data pulled from the dataset to find accurate values for the weights. Once the model has been trained, it is tested against new data find its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN(x):\n",
    "\n",
    "    # Setup Model\n",
    "    prediction = neural_net(x)\n",
    "\n",
    "    #Produce Cost Function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "\n",
    "    # Optimizer\n",
    "    optimize = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Number of Back Prop Cycles\n",
    "    cycles = 10\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for cycle in range(cycles):\n",
    "            cycle_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = batch_size+i\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                _, c = sess.run([optimize,cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                cycle_loss += c\n",
    "                i += batch_size\n",
    "\n",
    "            print (\"Epoch\", cycle+1, \"completed out of\" , cycles)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print ('Accuracy', accuracy.eval({x:test_x, y:test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the model and test it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, 'completed out of', 10)\n",
      "('Epoch', 2, 'completed out of', 10)\n",
      "('Epoch', 3, 'completed out of', 10)\n",
      "('Epoch', 4, 'completed out of', 10)\n",
      "('Epoch', 5, 'completed out of', 10)\n",
      "('Epoch', 6, 'completed out of', 10)\n",
      "('Epoch', 7, 'completed out of', 10)\n",
      "('Epoch', 8, 'completed out of', 10)\n",
      "('Epoch', 9, 'completed out of', 10)\n",
      "('Epoch', 10, 'completed out of', 10)\n",
      "('Accuracy', 0.98000002)\n"
     ]
    }
   ],
   "source": [
    "train_NN(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model performed well against the tested data, however, it is slightly off due to likely overfitting. This is still a good example of how deep learning models can be used to predict values given a set of data and, if used in the final model, will help to predict patterns that may have been difficult to model by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
